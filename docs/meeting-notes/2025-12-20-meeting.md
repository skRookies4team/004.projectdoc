# 프로젝트 회의록

## 회의 정보
- **회의명**: 프로젝트 회의
- **일시**: 2025.12.20
- **장소**: 강의실 394
- **회의 유형**: 기획회의

## 참석자
- **진행자**: 팀장 최대현
- **참석자**: 
  - 국영규
  - 강소현
  - 김세희
  - 모인지  
  - 윤종윤
  - 이유진
  - 임성현
  - 최대현  
  - 이도원
- **불참자**:

## 회의 안건
1. 임베딩 후보: ko-sroberta-multitask vs GTE-Qwen2(1.5B/7B) 성능(정확도/속도/비용) 비교
2. 오픈 AI/제미나이 계열 모델 및 보안성 높은 모델 비교
3. 밀버스(Milvus)·오픈 API 텍스트 라지 모델 사용 현황
4. 새로운 모델 도입 시점과 품질 검증 필요성
5. 품질 vs 보안 전략 및 AB 테스트 개념
6. 클라우드 vs 온프레미스, CBT(시범 테스트) 도입
7. AI 프로젝트에서의 프론트 관리와 거버넌스 채팅
8. 모델 선택 UI 및 사용자 컨트롤 제공
9. 플랫폼·일반화 관점에서의 프로젝트 목표
10. 테스트 종류 및 정확도 개선을 위한 로그 수집 논의
11. AI 파이프라인 완성 및 남은 과제(품질 개선·로그 전략)
12. 로그 범위·포맷·저장 위치에 대한 설계 쟁점
13. CloudFront 기반 배포 및 백엔드 분리 이유
14. 클라우드 인프라 전환 시 영향 범위
15. 비즈니스 로직 vs 공통 로직(로그·인증·트랜잭션) 분리 필요성
16. AI 서버 로그 포맷·스토리지 활용 방안
17. 로그 전용 서비스(로그 Edge 서비스) 및 ELK 스택 도입 검토
18. CloudWatch 및 CI/CD와 연계한 관리
19. 비동기 처리와 실패 케이스 설계
20. 개인화 로직에서 키워드 매칭 + LLM 역할 분리
21. 답변 유형 분류와 API 설계

## 회의 내용

### 안건 1: 임베딩 후보: ko-sroberta-multitask vs GTE-Qwen2(1.5B/7B) 성능(정확도/속도/비용) 비교
- **논의 내용**:
  - 여러 모델을 테스트한 결과, 기존에 사용하던 GTE-Qwen2만 쓸 때보다 sroberta 사용 시 성능이 더 좋아서 현재는 sroberta를 사용
  - 전반적으로 응답 품질·정확도 측면에서 sroberta가 더 유리
  - sroberta를 기준 모델로 삼되,이후 다른 모델과의 비교를 위한 측정 지표(정확도·체감 품질 등) 정립 필요

### 안건 2: 오픈 AI/제미나이 계열 모델 및 보안성 높은 모델 비교
- **논의 내용**: 
  - 오픈 AI 계열 / 제미나이(Gemini) 모델은 성능·정확도 측면에서 가장 우수한 축에 속한다는 평가
  - 반면, 보안성이 강화된 3번째 옵션(온프레미스·내부 배포형 등)은 정확도도 나쁘지 않고 보안도 좋지만, 구축·운영에 공수가 한 달 이상 걸릴 수 있는 부담이 있음
  - “품질 이슈가 있는데, 보안성만의 이유로 모델을 쓰는 게 맞는지?”
    → 보안 vs 품질 트레이드오프를 팀 차원에서 명확히 논의·정리할 필요

### 안건 3: 밀버스(Milvus)·오픈 API 텍스트 라지 모델 사용 현황
- **논의 내용**:
  - 벡터DB로 밀버스(Milvus)를 사용하고 있으며, 테스트 단계에서는 제미나이를 이용해 RAG 질의 테스트를 수행
  - 실제 적재 시에는 오픈 API 텍스트 라지 모델로 임베딩을 구성했던 이력이 있음
  - 언급된 OSS 모델이 임베딩 전용 모델인지 확실하지 않아 품질을 장담하기 어려움
  - 보안만 강조한다고 해서 현재 품질이 떨어지는 선택을 정당화할 수 없음
  - 모델별로 보안·성능·운영 난이도를 함께 보고 “품질을 어느 수준까지 허용할 수 있는지” 최소 기준치(Threshold) 설정 필요

### 안건 4: 새로운 모델 도입 시점과 품질 검증 필요성
- **논의 내용**:
  - 프로젝트 진행률은 전체 WBS 기준 약 79% 지점에 도달한 상황으로 언급됨
  - 남은 아이템(WBS 상 잔여 작업)은 여전히 많지만, 지금 시점에 새로운 모델이 등장했을 경우 품질이 충분히 검증된다면 도입을 검토할 만함 그러나, “품질을 모르는 상태에서 보안 때문에 손쉽게 갈아타는 것”은 위험
  - 새 모델 도입 시에는 사전 품질 검증(샘플 질의, 평가 지표, 체감 평가) 절차를 필수로 거쳐야 한다는 점 확인

### 안건 5: 품질 vs 보안 전략 및 AB 테스트 개념
- **논의 내용**:
  - 제미나이 클라우드 환경을 사용할 것인지, sroberta를 온프레미스 환경에서 활용할 것인지를 결정할 필요가 있음
  - 클라우드 사용 시 기간을 한정한 시범 서비스(CBT, Closed Beta Test) 형태로 실제 사용자/시나리오를 돌려보고 결과를 기반으로 최종 아키텍처·모델 선정에 반영하는 방향 제안
  - AB 테스트와 CBT를 조합해 시스템을 능동적으로 개선해 나가는 “서비스 운영 관점”까지 가져가야 함
  - 단순 PoC가 아니라, “실제 운영을 염두에 둔 실험·검증 루프”를 설계

### 안건 6: 클라우드 vs 온프레미스, CBT(시범 테스트) 도입
- **논의 내용**:
  - 벡터DB로 밀버스(Milvus)를 사용하고 있으며, 테스트 단계에서는 제미나이를 이용해 RAG 질의 테스트를 수행
  - 실제 적재 시에는 오픈 API 텍스트 라지 모델로 임베딩을 구성했던 이력이 있음
  - 언급된 OSS 모델이 임베딩 전용 모델인지 확실하지 않아 품질을 장담하기 어려움
  - 보안만 강조한다고 해서 현재 품질이 떨어지는 선택을 정당화할 수 없음
  - 모델별로 보안·성능·운영 난이도를 함께 보고 “품질을 어느 수준까지 허용할 수 있는지” 최소 기준치(Threshold) 설정 필요

### 안건 7: AI 프로젝트에서의 프론트 관리와 거버넌스 채팅
- **논의 내용**:
  - 벡터DB로 밀버스(Milvus)를 사용하고 있으며, 테스트 단계에서는 제미나이를 이용해 RAG 질의 테스트를 수행
  - 실제 적재 시에는 오픈 API 텍스트 라지 모델로 임베딩을 구성했던 이력이 있음
  - 언급된 OSS 모델이 임베딩 전용 모델인지 확실하지 않아 품질을 장담하기 어려움
  - 보안만 강조한다고 해서 현재 품질이 떨어지는 선택을 정당화할 수 없음
  - 모델별로 보안·성능·운영 난이도를 함께 보고 “품질을 어느 수준까지 허용할 수 있는지” 최소 기준치(Threshold) 설정 필요

### 안건 8: 모델 선택 UI 및 사용자 컨트롤 제공
- **논의 내용**:
  - 사용자가 모델 또는 모드(정확도 우선 / 속도 우선 / 보안 우선 등)를 선택할 수 있게 하면, 개발·실험을 빠르게 진행할 수 있고, 동시에 “어떤 모드에서 어떤 결과가 나오는지” 데이터를 축적 가능
  - 단순히 기능 옵션을 늘리려는 것이 아니라, “이런 프로세스(선택 → 실험 → 결과 관찰)를 설계해 보라”는 데 있음
  - 실제 사용자에게 어느 정도까지 선택권을 줄지, 내부 실험용 옵션과 사용자용 옵션을 어떻게 구분할지 추가 설계 필요

### 안건 9: 플랫폼·일반화 관점에서의 프로젝트 목표
- **논의 내용**:
  - “이 프로젝트의 최종 목표가 무엇인가?”를 다시 상기할 것
  - 단일 기업용 PoC로 끝내는 것이 아니라, 여러 기업·도메인에서 범용적으로 사용할 수 있는 형태로 일반화(플랫폼화)하는 과정이 중요
  - 즉, 특정 회사용 맞춤 기능에만 집중하기보다 “플랫폼으로 발전 가능한 구조·설계”를 의식하고 진행
  - 최종 발표/문서에서 “AI 교육·컴플라이언스 어시스턴트 플랫폼”이라는 포지셔닝을 명시적으로 담아둬야 함

 ### 안건 10: 테스트 종류 및 정확도 개선을 위한 로그 수집 논의
- **논의 내용**:
  - 모델 정확도를 높이기 위한 후속 작업(튜닝·개선 액션)이 필요하다는 전제에서, “개선 전/후 수치 차이”를 확인할 수 있는 테스트 기준이 무엇인지 논의
  - 로그 수집 방향 초안: 채팅 로그, 시스템 로그를 AI 서버에 JSON 형식으로 저장하는 방식 검토 중
  - 단순히 로그를 쌓는 것이 아니라, “어떤 테스트를 기준으로 개선 효과를 측정할지” 먼저 정의 필요
  
### 안건 11: AI 파이프라인 완성 및 남은 과제(품질 개선·로그 전략)
- **논의 내용**:
  - 현재 기준: AI 파이프라인(요청 → RAG → 응답 생성) 자체는 1차적으로 완성된 상태로 공유
  - 향후 핵심 과제는 품질 개선(정확도·일관성 향상) 및 이를 뒷받침할 로그 수집·분석 체계 설계로 정리
  - “로그를 어디에서 남길 것인지?” 프론트(클라이언트) 레벨인지, 백엔드/AI 서버 레벨인지, 혹은 양쪽 모두인지 관점 정리가 필요함
  - 로그 위치에 따라 수집 정보와 활용 가능성이 달라지므로, “프론트·백엔드·AI 서버별 로그 역할” 정리 필요  

### 안건 12: 로그 범위·포맷·저장 위치에 대한 설계 쟁점
- **논의 내용**:
  - 로그 설계 시 아래 항목을 고려해야 함
  - 로그 범위/타깃 정의: 어떤 이벤트를 남길 것인지(요청/응답, 에러, 프롬프트, RAG 검색 결과 등)
  - 로그 포맷: 공통 JSON 스키마로 맞출지, 서비스별 스키마를 둘지
  - 저장 위치·방식: 단순 파일 시스템/스토리지에 저장할지, 클라우드 서비스(예: CloudWatch, S3)를 사용할지, 별도 로그 전용 인프라를 둘지
  - 현재는 기존 서비스에서 제공하는 로그 기능을 우선 활용함
  - 장기적으로는 로그 포맷을 표준화하여 운영툴·대시보드·분석 도구와 쉽게 연동할 수 있도록 해야 함

### 안건 13: CloudFront 기반 배포 및 백엔드 분리 이유
- **논의 내용**:
  - 프론트/백엔드 배포 구조 관련 논의: 정적/비정적 여부를 떠나, CloudFront를 통해 프론트를 배포하고, 백엔드 서비스는 별도로 분리하는 구성 사용 중
  - 패싱된 데이터를 그대로 받아 전달하기 쉽고, 추후 확장·관리 측면에서 이점이 있기 때문에 CloudFront 선택
  - 관리성·운영 편의성 측면에서 CloudFront 구성이 더 낫다고 판단했기 때문에 다른 대안(예: 기존 “쿠버네티스” 기반 배포)을 쓰지 않음
  - 인프라 구성이 고정되기 전에 “현재 구조를 아키텍처 다이어그램으로 명확히 시각화” 해둘 필요 있음

### 안건 14: 클라우드 인프라 전환 시 영향 범위
- **논의 내용**:
  - 서버를 AWS가 아닌 다른 인프라(예: ‘레저’ 환경)로 바꾸면 어디를 수정해야 하는지 논의
  - VPC, 네트워크, 게이트웨이 등 인프라 계층의 설정 변경이 필요하며, 이로 인해 서비스·파이프라인 전체 구성에 영향을 줄 수 있음
  - 이런 변경을 수월하게 하기 위해 앞단 설계(데이터 경로, 서비스 경계, 게이트웨이 등)를 추상화·일관화하는 것이 중요함
  - 인프라 전환 가능성을 염두에 두고 “환경 의존성이 낮은 구조”를 설계하는 것이 필요

### 안건 15: 비즈니스 로직 vs 공통 로직(로그·인증·트랜잭션) 분리 필요성
- **논의 내용**:
  - 로그 관리, 인증 관리, 트랜잭션 관리 등은 비즈니스 로직과 별개의 공통 기능이며, 이를 마이크로 서비스에서 분리·공통화하여 관리하는 것이 핵심
  - 따라서, 비즈니스 로직(예: 교육 추천, 퀴즈 채점, 개인화 리포트)과 공통 인프라 로직(로그·인증·트랜잭션)은 명확히 분리 설계해야 함
  - 현재 코드·설계 상에서 두 영역이 섞여 있지 않은 지 점검 필요

### 안건 16: AI 서버 로그 포맷·스토리지 활용 방안
- **논의 내용**:
  - AI 서버에서의 로그 처리 논의: 이미 AI 서버 내부 스토리지에 로그를 남기고 있으며, JSON 형태로 남기기 때문에 이후 다른 스토리지나 분석 시스템으로 옮겨 활용하는 것도 가능함
  - 로그 대시보드는 현재 사용 중인 AI 서버/플랫폼이 자체 제공하는 화면을 활용 중
  - 단기적으로는 내부 대시보드를 활용하되, 장기적으로는 사내 대시보드(운영 관제)에 통합하는 방향 검토 필요

### 안건 17: 로그 전용 서비스(로그 Edge 서비스) 및 ELK 스택 도입 검토
- **논의 내용**:
  - 로그는 메인 시스템과 분리된 독립 서비스(“로그 Edge 서비스”)로 운영하는 것이 바람직함
  - 오픈소스 로그 스택 예시로, ELK 스택(Elasticsearch + Logstash + Kibana 유사 구성), 기타 로그 수집·검색·시각화 도구 등을 언급하며 이러한 도구를 적극적으로 활용할 것을 권장
  - 최종 서비스에서는 “로그 수집 → 적재 → 검색/필터링 → 시각화”가 하나의 체계로 동작

### 안건 18: CloudWatch 및 CI/CD와 연계한 관리
- **논의 내용**:
  - AWS CloudWatch의 역할: 요청·에러 등의 기본 로그 수집 및 모니터링 용도로 활용 가능
  - 다만, 장기적으로는 로그를 별도 서비스로 분리하고, CI/CD 파이프라인(CR/CD)과 함께 배포·로그·모니터링을 일원화하는 구성이 이상적
  - 현재는 MVP 단계이므로CloudWatch + 단일 로그 저장소로 시작하되, 향후 MSA·CI/CD 고도화 시 로그 서비스 분리 고려

### 안건 19: 비동기 처리와 실패 케이스 설계
- **논의 내용**:
  - 비동기 처리(예: 비동기 API 호출, 비동기 작업 큐)를 사용할 경우, 실패 케이스를 어떻게 처리할지에 대한 설계가 필수
  - 단순 성공 플로우만 정의하는 것이 아니라, 응답 지연, 오류, 재시도 실패 등 각종 예외 상황에 대한 처리 정책을 명확히 설계
  - 현재 시스템 설계 문서·시퀀스 다이어그램에 “실패 시 동작(에러 메시지, 재시도, 롤백 등)”이 충분히 표현되지 않았을 가능성 있음

### 안건 20: 개인화 로직에서 키워드 매칭 + LLM 역할 분리
- **논의 내용**:
  - 개인화 영역에서 1차 키워드 매칭(규칙 기반)으로 후보를 좁힌 뒤, 경계값(스코어가 애매한 영역)에 있는 케이스는 LLM이 최종 판단하도록 하는 구조 설명
  - 같은 질문이라도 상황·맥락에 따라 의도가 달라질 수 있기 때문에, LLM이 최종적으로 의도를 해석하지만, 불필요한 호출을 줄이기 위해 사전 로직(키워드·스코어 기반 필터링)을 두는 전략
  - 개인화 시나리오(예: 교육 추천, 근태·복지 조회)에 대해 “키워드/룰로 처리되는 영역 vs LLM이 판단하는 영역”을 시각화해 둘 필요 있음

### 안건 21: 답변 유형 분류와 API 설계
- **논의 내용**:
  - 답변 유형(예: 단답, 요약, 리스트, 경고, 액션 안내 등)을 분류하는 것도 중요
  - 하나의 API 엔드포인트를 사용하더라도, 질문 유형에 따라 응답 타입(필드 구조, 컬럼 구성)이 달라질 수 있음
  - 현재 설계에서는 컬럼(필드) 차이를 충분히 고려하지 못한 부분이 있다는 피드백
  - “질문 유형 → 응답 구조 타입 매핑표”를 정의해 두면, 프론트·백엔드·LLM 프롬프트 설계에 모두 도움이 될 것

- **결정 사항**: 
  - 모델 후보·비교 기준 정리: sroberta, GTE-Qwen2, 오픈 AI/제미나이, 보안성 높은 온프레미스 모델(3rd 옵션) 간의 보안·정확도·응답속도·구축 난이도 비교표를 작성 (AI / Infra)(2025-12-21~2025-12-24)
  - AB 테스트 전략 초안 수립: A/B 두 개 이상의 모델·설정에 대해 실험 시나리오, 평가 지표, 롤백 기준을 포함한 AB 테스트 전략 문서를 작성 (AI / PM) (2025-12-21~2025-12-27)
  - CBT(시범 서비스) 설계: 제미나이 클라우드 환경 또는 sroberta 온프레미스 기반으로 단기 CBT(시범 운영) 플로우를 설계하고, 필요한 계정·리소스를 정리 (Infra / BM) (2025-12-22~2025-12-29)
  - 프론트·모델 선택 UI 기획: 사용자가 선택 가능한 모델/모드(정확도 우선, 속도 우선, 보안 우선 등) UI와, 내부 실험용 플래그 구조에 대한 와이어프레임·기획서를 작성 (FE / 기획) (2025-12-23~2025-12-30)
  - 플랫폼·일반화 방향 문서화: 프로젝트를 플랫폼 형태로 일반화하기 위한 방향(다른 기업·도메인 적용 시나리오)을 정리해 중간/최종 발표 개요에 반영 (PM / 기획) (2025-12-24~2025-12-31)
  - 로그 전략 초안 수립: 채팅 로그·시스템 로그를 JSON 포맷으로 수집하는 기본 전략을 전제로, 로그 범위·포맷·저장 위치(스토리지/CloudWatch 등)를 1차 문서로 정리 (BE / AI) (2025-12-21~2025-12-24)
  - 로그 활용 시나리오 정의: 수집한 로그를 활용한 정확도·품질 개선 프로세스(전/후 비교, 측정 지표) 를 시나리오 형태로 정리 (AI / PM)(2025-12-22~2025-12-26)
  - 로그 서비스 분리 검토: 향후 로그 전용 서비스(로그 Edge 서비스) 및 ELK 등 오픈소스 스택 도입 가능성을 검토하고, 아키텍처 초안을 작성 (Infra / BM) (2025-12-23~2025-12-30)
  - 실패 케이스 설계 보완: 비동기 처리 구간(예: API 호출, 큐 처리 등)에 대해 실패 시나리오(에러 처리, 재시도, 사용자 안내)를 정의하고, 시퀀스 다이어그램/문서에 반영 (BE / AI)(2025-12-21~2025-12-24)
  - 개인화·답변 유형 설계 명문화: 키워드 매칭 + LLM 판단 플로우 및 질문 유형별 응답 타입(구조/필드) 을 매핑표와 간단한 다이어그램으로 정리 (AI / 기획) (2025-12-21~2025-12-26)

## 다음 회의
- **일시**: 2025-12-23

## 기타 사항
- 모든 회의록은 GitHub의 docs/meeting-notes/ 디렉토리에 저장
- 회의록은 회의 종료 후 24시간 내에 공유

---
**작성자**: 이유진  
**작성일**: 2025-12-20