# 프로젝트 회의록

## 회의 정보
- **회의명**: 프로젝트 회의
- **일시**: 2025.12.08
- **장소**: 강의실 372
- **회의 유형**: 기획회의

## 참석자
- **진행자**: 팀장 최대현
- **참석자**: 
  - 국영규
  - 강소현
  - 김세희
  - 모인지  
  - 윤종윤
  - 이유진
  - 임성현
  - 최대현  
  - 이상호
- **불참자**:

## 회의 안건
1. API 개발 현황 및 AI 성능 테스트 진행 상황
2. AI 연동 일정 및 로컬 데모 목표
3. 보안 및 LLM 운영 방식: 외부 호출 vs 온프레미스
4. 서버 사이즈·자원 사용·클라우드 설정(배포) 계획

## 회의 내용

### 안건 1: API 개발 현황 및 AI 성능 테스트 진행 상황
- **논의 내용**:
  - 프론트엔드 영역을 제외하고 볼 때, 백엔드 API 개발량은 많지 않은 편으로 판단
  - 전체 API 개수는 약 50~60개 정도 수준으로 예상
  - 현재 백엔드 코드 자체는 큰 이슈 없이 진행 중
  - AI 쪽은 성능 테스트를 진행 중이며, 모델 후보들을 비교·검증하는 단계
  - API 수량·범위가 현실적인 수준인지 재확인 필요(필수/선택 기능 구분)
  - AI 성능 테스트 결과를 언제, 어떤 형태(지표/표/그래프)로 정리할지 일정화가 필요

### 안건 2: AI 연동 일정 및 로컬 데모 목표
- **논의 내용**: 
  - 이번 주 안에 AI까지 로컬 환경에서 연동한 상태를 보여주기 위해서 최대한 이번 주 내에 로컬 연동 데모를 준비
  - 이번 주 안에 백엔드–AI 연동이 된 상태에서 실제 동작하는 화면을 직접 보여줘야 함
  - 현재 기준으로는 AI 연동 자체에 큰 기술적 이슈는 없는 상태
  - 이번 주(12/8~12/13) 내에 “최소 동작 가능한 로컬 데모(MVP 수준)”를 목표로 일정·우선순위를 재조정 필요

### 안건 3: 보안 및 LLM 운영 방식: 외부 호출 vs 온프레미스
- **논의 내용**:
  - 당초에는 외부 GPT/LLM API를 사용하는 방향으로 이해하고 있었으나, 실제로는 그게 아니라는 점이 명확해짐
  - 이유: 외부로 나갈 경우 성능(속도)의 저하와 보안 측면(데이터 외부 전송)에 대한 리스크가 있음
  - 결론적으로, 외부 GPT/LLM 호출은 사용하지 않고 온프레미스(내부 인프라) 환경 내에서 모델을 운영하는 방향으로 정리
  - 아키텍처/보안 문서에서 “외부 LLM 호출 → 내부 온프레미스 LLM 운영”으로 방향이 바뀐 부분을 명시적으로 반영

### 안건 4: 서버 사이즈·자원 사용·클라우드 설정(배포) 계획
- **논의 내용**:
  - 현재 서버 사이즈와 자원(CPU, 메모리 등) 사용량을 체크 중
  - 성능 테스트 결과, 가장 잘 나오는 모델 후보가 생각보다 무겁지 않은 모델이라 자원 측면에서는 큰 부담이 없을 것으로 예상
  - 클라우드는 풀매니지드 대규모 구성이 아니라 “배포용 정도 규모”로 설정해도 충분할 것
  - 클라우드 환경을 지금부터 미리 설정해 두는 이유는, 나중에 시작하려고 할 때 인프라 세팅 때문에 막히지 않도록 사전에 기본 틀을 구성해 두기 위함이라고 설명
  - EKS 등 클러스터/모델 배포 연습을 먼저 해보면, 생각보다 모델 올리는 과정 자체는 어렵지 않을 것
  - 인프라 팀에서 “최소 필요 스펙 + 여유 리소스” 기준으로 서버/클러스터 사이즈를 산정해 둘 필요 있음
  - 클라우드 설정은 “지금부터 천천히 세팅 → 데모 시점에 바로 활용” 전략으로 가져가는 게 적절

- **결정 사항**: 
  - 로컬 AI 연동 데모 준비: 백엔드–AI 연동을 로컬 환경에서 완료하고, 기본 질문·응답이 가능한 최소 데모 화면을 준비 (BE / AI) (2025-12-08~2025-12-13)
  - AI 성능 테스트 정리: 후보 모델(내부 LLM 등)에 대해 응답 속도·정확도 등 핵심 지표를 테스트하고, 비교 결과를 표/그래프 형태로 정리 (AI) (2025-12-08~2025-12-16)
  - 온프레미스 LLM 운영 방향 문서화: 외부 GPT/LLM 호출을 사용하지 않고, 온프레미스 환경 내 LLM 운영으로 결정된 내용을 아키텍처·보안 문서 및 발표 자료에 반영 (AI / Infra) (2025-12-09~2025-12-14)
  - 서버 사이즈·자원 사용 기준 확정: 현재 테스트 중인 모델 기준으로 CPU/메모리 사용량을 확인하고, 클러스터/서버의 최소 스펙과 권장 스펙을 정리한다 (Infra) (2025-12-09~2025-12-17)
  - 클라우드(EKS 등) 기본 환경 세팅: 데모·배포를 대비해 EKS 등 클러스터 생성·배포 연습을 진행하고, 간단한 샘플 앱/모델을 올려보는 테스트를 수행 (Infra) (2025-12-10~2025-12-18)
  
## 다음 회의
- **일시**: 2025-12-09

## 기타 사항
- 모든 회의록은 GitHub의 docs/meeting-notes/ 디렉토리에 저장
- 회의록은 회의 종료 후 24시간 내에 공유

---
**작성자**: 이유진  
**작성일**: 2025-12-08